{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3b25b3-2124-4ee6-8e53-518799891d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.5\"\n",
    "import io, time, torch, datasets, PIL.Image, numpy as np, matplotlib.pyplot as plt, fastprogress, IPython.display, pillow_jpls\n",
    "from einops.layers.torch import Rearrange\n",
    "from types import SimpleNamespace\n",
    "from typing import OrderedDict\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from torchvision.transforms import Compose, Resize, RandomCrop, CenterCrop, ColorJitter\n",
    "from attend.asym import ConvND, ConvTransposeND, AsymptoticSphereNorm, GELUTanh, Quantize, VitBlockND\n",
    "from timm.optim import Adan\n",
    "from livecodec.codec import latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d93e41-4598-49bd-83f9-3ad45fa99c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c0105f46514add9bb357f381a7e2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640f58fb2ce4405d83b88716e8531df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3b512b03bf41c7a770b2c3134792b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.load_dataset(\"danjacobellis/LSDIR\",split='train'),\n",
    "    'validation': datasets.load_dataset(\"danjacobellis/kodak\",split='validation')\n",
    "})\n",
    "config = SimpleNamespace()\n",
    "config.input_dim = 2\n",
    "config.input_channels = 3\n",
    "config.ps = 16\n",
    "config.dr = 16\n",
    "config.use_wpt = False\n",
    "config.latent_dim = int(config.input_channels*(config.ps**config.input_dim)/config.dr)\n",
    "config.decoder_depth = 16\n",
    "config.hidden_dim = 768\n",
    "config.λ = 3e-2\n",
    "config.freeze_encoder_after = 0.7\n",
    "config.epochs = 50\n",
    "config.batch_size = 4\n",
    "config.total_steps = config.epochs * (dataset['train'].num_rows // config.batch_size)\n",
    "config.min_aspect = 0.5\n",
    "config.max_aspect = 2\n",
    "config.min_size = 240\n",
    "config.max_size = 640\n",
    "config.min_scale = 1.0\n",
    "config.max_scale = 2.0\n",
    "config.max_lr = 3e-4\n",
    "config.min_lr = 1e-8\n",
    "config.lr_pow = 2\n",
    "config.num_workers = 12\n",
    "config.save_checkpoint_name = f'checkpoint_{device}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df51d1-59b7-4bd6-b780-0a7a9faf0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class V3(torch.nn.Module):\n",
    "    def __init__(self, dim, ch, ps, latent_dim, hidden_dim, decoder_depth, use_wpt):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ch = ch\n",
    "        self.ps = ps\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder_depth = decoder_depth\n",
    "        self.J = int(np.log2(ps))\n",
    "        self.patch_dim = ch*(ps**dim)\n",
    "        stride = 1 if use_wpt else ps\n",
    "        if dim == 1:\n",
    "            from tft.wavelet import WPT1D, IWPT1D, DWT1DForward, DWT1DInverse\n",
    "            self.wt = DWT1DForward(J=1, wave='bior4.4')\n",
    "            self.wpt = WPT1D(wt=self.wt, J=self.J)\n",
    "            self.iwt = DWT1DInverse(wave='bior4.4')\n",
    "            self.iwpt = IWPT1D(iwt=self.iwt, J=self.J)\n",
    "            conv_layer = torch.nn.Conv1d\n",
    "        elif dim == 2:\n",
    "            from tft.wavelet import WPT2D, IWPT2D, DWT2DForward, DWT2DInverse\n",
    "            self.wt = DWT2DForward(J=1, wave='bior4.4')\n",
    "            self.wpt = WPT2D(wt=self.wt, J=self.J)\n",
    "            self.iwt = DWT2DInverse(wave='bior4.4')\n",
    "            self.iwpt = IWPT2D(iwt=self.iwt, J=self.J)\n",
    "            conv_layer = torch.nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            from tft.wavelet import WPT3D, IWPT3D, DWT3DForward, DWT3DInverse\n",
    "            self.wt = DWT3DForward(J=1, wave='bior4.4')\n",
    "            self.wpt = WPT3D(wt=self.wt, J=self.J)\n",
    "            self.iwt = DWT3DInverse(wave='bior4.4')\n",
    "            self.iwpt = IWPT3D(iwt=self.iwt, J=self.J)\n",
    "            conv_layer = torch.nn.Conv3d\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"analysis_transform\", torch.nn.Sequential(\n",
    "                        OrderedDict(\n",
    "                            [\n",
    "                                (\"wpt\", self.wpt if use_wpt else torch.nn.Identity()),\n",
    "                                (\"norm\", AsymptoticSphereNorm() if use_wpt else torch.nn.Identity()),\n",
    "                                (\"act\", torch.nn.Identity()),\n",
    "                                (\"nn\", ConvND(self.dim, self.patch_dim if use_wpt else ch, self.latent_dim, kernel_size=stride, stride=stride, padding=0)),\n",
    "                            ]\n",
    "                        )\n",
    "                    )),\n",
    "                    (\"norm\", AsymptoticSphereNorm()),\n",
    "                    (\"quantize\", Quantize()),\n",
    "                    (\"synthesis_transform\", torch.nn.Sequential(\n",
    "                        OrderedDict(\n",
    "                            [\n",
    "                                (\"nn\", torch.nn.Sequential(\n",
    "                                    ConvND(self.dim, self.latent_dim, config.hidden_dim, kernel_size=1, stride=1, padding=0),\n",
    "                                    *[VitBlockND(dim, in_channels=config.hidden_dim, norm_layer=AsymptoticSphereNorm, act_layer=GELUTanh, quant_layer=torch.nn.Identity, head_dim=32, expand_ratio=2, drop_path=0.0)\n",
    "                                      for _ in range(config.decoder_depth)],\n",
    "                                    ConvTransposeND(self.dim, self.hidden_dim, self.patch_dim if use_wpt else ch, kernel_size=stride, stride=stride, padding=0),\n",
    "                                )),\n",
    "                                (\"iwpt\", self.iwpt if use_wpt else torch.nn.Identity()),\n",
    "                            ]\n",
    "                        )\n",
    "                    )),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        \n",
    "    def forward_rate(self, x):\n",
    "        z = self.layers[:3](x)\n",
    "        rate = z.std().log2()\n",
    "        xhat = self.layers[-1](z)\n",
    "        return xhat, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae672c6a-55c1-438c-b83a-1e11e3109c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = V3(dim=2, ch=config.input_channels, ps=config.ps, latent_dim=config.latent_dim, hidden_dim=config.hidden_dim, decoder_depth=config.decoder_depth, use_wpt = config.use_wpt)\n",
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6)\n",
    "\n",
    "optimizer = Adan(model.parameters(), lr=1.0)\n",
    "def rc_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return (config.max_lr - config.min_lr) * (1 - ((np.cos(np.pi*t))**(2*config.lr_pow))) + config.min_lr\n",
    "schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda i_step: rc_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edfcd4-6943-47fb-8828-b079422d1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(batch):\n",
    "    aspect = np.random.uniform(config.min_aspect,config.max_aspect)\n",
    "    h = np.random.uniform(config.min_size,config.max_size); w = h*aspect;\n",
    "    h = int((config.ps*(np.rint(h/config.ps))).clip(config.min_size,config.max_size))\n",
    "    w = int((config.ps*(np.rint(w/config.ps))).clip(config.min_size,config.max_size))\n",
    "    x = []\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        transform = Compose([\n",
    "            Resize(\n",
    "                int(max(h,w)*np.random.uniform(config.min_scale,config.max_scale)),\n",
    "                interpolation=PIL.Image.Resampling.BICUBIC,\n",
    "                max_size=1+int(config.max_scale*config.max_size)\n",
    "            ),\n",
    "            RandomCrop((h,w),pad_if_needed=True,padding_mode='symmetric'),\n",
    "            ColorJitter(0.4,0.0,0.4,0.0),\n",
    "        ])\n",
    "        xi = pil_to_tensor(transform(sample['image'].convert(\"RGB\"))).unsqueeze(0)\n",
    "        x.append(xi)\n",
    "    x = torch.cat(x)\n",
    "    return x.to(torch.float)/127.5 - 1.0\n",
    "\n",
    "def enc_dec(sample):\n",
    "    img = sample['image']\n",
    "    buff = io.BytesIO()\n",
    "    x = pil_to_tensor(img).to(torch.float).to(device).unsqueeze(0)/127.5 - 1.0\n",
    "    t0 = time.time()\n",
    "    with torch.inference_mode():\n",
    "        z = model.layers[0:2](x).round()\n",
    "    latent_to_pil(z.cpu().detach(),n_bits=8,C=3)[0].save(buff, format='JPEG-LS')\n",
    "    enc_time = time.time() - t0\n",
    "    size_bytes = len(buff.getbuffer())\n",
    "    t0 = time.time()\n",
    "    z = pil_to_latent([PIL.Image.open(buff)],N=config.latent_dim,n_bits=8,C=3).to(device)\n",
    "    with torch.inference_mode():\n",
    "        xhat = model.layers[-1](z).clamp(-1,1)\n",
    "    dec_time = time.time() - t0\n",
    "    x_01 = x/2 + 0.5\n",
    "    xhat_01 = xhat/2 + 0.5\n",
    "    psnr = -10*torch.nn.functional.mse_loss(x_01, xhat_01).log10().item()\n",
    "    del sample['path']\n",
    "    del sample['image']\n",
    "    return{\n",
    "        'psnr': psnr,\n",
    "        'cr': x.numel()/size_bytes,\n",
    "        'enc': (x.numel()/3e6)/enc_time,\n",
    "        'dec': (x.numel()/3e6)/dec_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cdeed-fe20-4489-8148-c3fea3893419",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "mb = fastprogress.master_bar(range(config.epochs))\n",
    "log_mse_losses = []\n",
    "rate_losses = []\n",
    "valid_metrics = []\n",
    "global_step = 0\n",
    "for i_epoch in mb:\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "            dataset['train'],\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            collate_fn=train_collate_fn\n",
    "        )\n",
    "    pb = fastprogress.progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, x in enumerate(pb):\n",
    "        x = x.to(device);\n",
    "        xhat,rate = model.forward_rate(x)\n",
    "        log_mse_loss = torch.nn.functional.mse_loss(x,xhat).log10()\n",
    "        log_mse_losses.append(log_mse_loss.item())\n",
    "        rate_losses.append(rate.item())\n",
    "        total_loss = log_mse_loss + config.λ * rate\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0, norm_type=2.0)\n",
    "        optimizer.step()\n",
    "        schedule.step()\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        pb.comment = (f\"log mse: {log_mse_losses[-1]:.3g}, LR: {learning_rates[-1]:.2g}\")\n",
    "        global_step += 1\n",
    "\n",
    "    model.eval()\n",
    "    valid = dataset['validation'].map(enc_dec)\n",
    "    valid_metrics.append({key: torch.tensor(valid[key]).mean().item() for key in valid.features.keys()})\n",
    "    mb.main_bar.comment = \";  \".join([f'{key}: {valid_metrics[-1][key]:0.4g}' for key in valid_metrics[-1]])\n",
    "    \n",
    "    torch.save({\n",
    "        'i_epoch': i_epoch,\n",
    "        'log_mse_losses': log_mse_losses,\n",
    "        'rate_losses': rate_losses,\n",
    "        'learning_rates': learning_rates,\n",
    "        'valid_metrics': {k: [d[k] for d in valid_metrics] for k in valid_metrics[0].keys()},\n",
    "        'config': config,\n",
    "        'state_dict': model.state_dict()\n",
    "    }, config.save_checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809f76c-1d1b-4077-9677-f20a94bdd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPython.display.HTML(mb.main_bar.progress))\n",
    "display(IPython.display.HTML(pb.progress))\n",
    "N = 501; plt.plot(np.convolve(log_mse_losses, N*[1/N], mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede23a84-e594-46b8-8848-984f83089392",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(([[v[key] for key in v.keys()] for v in valid_metrics]))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055db6b-388d-40a7-bcb4-b368c3c8d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'i_epoch': i_epoch,\n",
    "    'log_mse_losses': log_mse_losses,\n",
    "    'rate_losses': rate_losses,\n",
    "    'learning_rates': learning_rates,\n",
    "    'valid_metrics': {k: [d[k] for d in valid_metrics] for k in valid_metrics[0].keys()},\n",
    "    'config': config,\n",
    "    'state_dict': model.state_dict()\n",
    "}, 'v3_p16_c12.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b31ff5-6788-42dc-888a-5ed46868750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for sample in dataset['validation'].select([4,6,12,22]):\n",
    "    img = CenterCrop(256)(sample['image'])\n",
    "    x = pil_to_tensor(img).to(torch.float).to(device).unsqueeze(0)/127.5 - 1.0\n",
    "    with torch.inference_mode():\n",
    "        z = model.layers[0:2](x).round()\n",
    "        buff = io.BytesIO()\n",
    "        latent_to_pil(z.cpu().detach(),n_bits=8,C=3)[0].save(buff, format='JPEG-LS')\n",
    "        size_bytes = len(buff.getbuffer())\n",
    "        xhat = model.layers[-1](z).clamp(-1,1)\n",
    "        psnr = -10*torch.nn.functional.mse_loss(x/2,xhat/2).log10()\n",
    "    display(img)\n",
    "    print(f'cr: {x.numel()/size_bytes}\\npsnr: {psnr.item()}')\n",
    "    display(to_pil_image(xhat[0]/2 + 0.5))\n",
    "    display(PIL.Image.open(buff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14251ce5-ffc1-42ad-8dc0-01f8c59ea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(z.cpu().flatten(),range=(-127.5,127.5),bins=255);\n",
    "plt.xlim([-64,64]);\n",
    "plt.xticks([-64,-32,-16,0,16,32,64]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10631d-b5ae-4e14-a347-26a3658ae5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for sample in dataset['validation'].select([4,6,12,22]):\n",
    "    img = sample['image']\n",
    "    x = pil_to_tensor(img).to(torch.float).to(device).unsqueeze(0)/127.5 - 1.0\n",
    "    with torch.inference_mode():\n",
    "        z = model.layers[0:2](x).round()\n",
    "        buff = io.BytesIO()\n",
    "        latent_to_pil(z.cpu().detach(),n_bits=8,C=3)[0].save(buff, format='JPEG-LS')\n",
    "        size_bytes = len(buff.getbuffer())\n",
    "        xhat = model.layers[-1](z).clamp(-1,1)\n",
    "        psnr = -10*torch.nn.functional.mse_loss(x/2,xhat/2).log10()\n",
    "    display(img)\n",
    "    print(f'cr: {x.numel()/size_bytes}\\npsnr: {psnr.item()}')\n",
    "    display(to_pil_image(xhat[0]/2 + 0.5))\n",
    "    display(PIL.Image.open(buff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1dca3b-1e6c-4b37-8f44-22d7cac01e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(z.cpu().flatten(),range=(-127.5,127.5),bins=255);\n",
    "plt.xlim([-64,64]);\n",
    "plt.xticks([-64,-32,-16,0,16,32,64]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g",
   "language": "python",
   "name": "g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
