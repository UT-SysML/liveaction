{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375d3181-5231-450d-ba9e-6b3ad9fc3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, timm, datasets, fastprogress, io, PIL.Image, pillow_jpls\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from livecodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1c6732-51c2-4f1b-80f3-3dfe7937c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/g/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/g/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "codec_device = 'cuda:1'\n",
    "device = 'cuda:0'\n",
    "ssim_loss = SSIMLoss().to(device)\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "psnr_db = lambda x01, xhat01: -10*torch.nn.functional.mse_loss(x01,xhat01).log10().item()\n",
    "ssim_01 = lambda x01, xhat01: 1 - ssim_loss(x01,xhat01).item()\n",
    "lpips_db = lambda x01, xhat01: -10*lpips_loss(x01,xhat01).log10().item()\n",
    "dists_db = lambda x01, xhat01: -10*dists_loss(x01,xhat01).log10().item()    \n",
    "\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/liveaction\",\n",
    "    filename=\"lsdir_f16c48_lambdap1.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "cconfig = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=cconfig.input_channels,\n",
    "    J = int(np.log2(cconfig.F)),\n",
    "    latent_dim=cconfig.latent_dim,\n",
    "    encoder_depth = cconfig.encoder_depth,\n",
    "    encoder_kernel_size = cconfig.encoder_kernel_size,\n",
    "    decoder_depth = cconfig.decoder_depth,\n",
    "    lightweight_encode = cconfig.lightweight_encode,\n",
    "    lightweight_decode = cconfig.lightweight_decode,\n",
    ").to(codec_device)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "config = SimpleNamespace()\n",
    "config.batch_size = 16\n",
    "config.num_workers = 8\n",
    "config.inference_size = 224\n",
    "\n",
    "model = timm.create_model('timm/eva_giant_patch14_224.clip_ft_in1k',pretrained=True).to(device)\n",
    "dataset = datasets.load_dataset('danjacobellis/imagenet_1k_val_224',split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1b4809-bdb1-48ef-9000-0e614b28414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, resize):\n",
    "    B = len(batch)\n",
    "    y = []\n",
    "    x = []\n",
    "    xr = []\n",
    "    for sample in batch:\n",
    "        y.append(sample['cls'])\n",
    "        img = sample['crop224']\n",
    "        x.append(pil_to_tensor(img).unsqueeze(0))\n",
    "        img = img.resize((resize,resize), resample=PIL.Image.Resampling.BICUBIC)\n",
    "        xr.append(pil_to_tensor(img).unsqueeze(0))\n",
    "    x = torch.cat(x)\n",
    "    xr = torch.cat(xr)\n",
    "    return x, xr, torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2ef66-f118-4762-b304-34e1da164117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='229' class='' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.33% [229/3125 01:23&lt;17:34 Acc: 0.8513]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G_A = lambda x: codec.quantize.compand(codec.encode(x)).round()\n",
    "resize_settings = [224]\n",
    "\n",
    "correct_matrix = []\n",
    "psnr_matrix = []\n",
    "ssim_matrix = []\n",
    "lpips_matrix = []\n",
    "dists_matrix = []\n",
    "size_matrix = []\n",
    "\n",
    "mb = fastprogress.master_bar(resize_settings)\n",
    "for i_r, resize in enumerate(mb):\n",
    "    dataloader = torch.utils.data.dataloader.DataLoader(\n",
    "        dataset=dataset,\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=lambda batch: collate_fn(batch, resize),\n",
    "        batch_size=config.batch_size,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    preds_c = []\n",
    "    size_bytes = []\n",
    "    psnr = []\n",
    "    ssim = []\n",
    "    lpips = []\n",
    "    dists = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pb = fastprogress.progress_bar(dataloader,parent=mb)\n",
    "    \n",
    "    for i_batch, (x, xr, y) in enumerate(pb):\n",
    "        y = y.to(device)\n",
    "        x = x.to(torch.float).to(device)/127.5 - 1.0\n",
    "        xr = xr.to(torch.float).to(codec_device)/127.5 - 1.0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            z = G_A(xr)\n",
    "            compressed = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "            sb = []\n",
    "            for i_sample in range(config.batch_size):\n",
    "                buff = io.BytesIO()\n",
    "                compressed[i_sample].save(buff, format='JPEG-LS')\n",
    "                sb.append(len(buff.getbuffer()))\n",
    "                del buff\n",
    "            size_bytes.append(sb)\n",
    "            xhat = codec.decode(z).to(device)\n",
    "            xhat = Resize(config.inference_size,interpolation=PIL.Image.Resampling.BICUBIC)(xhat).clamp(-1,1)\n",
    "\n",
    "        with torch.inference_mode(): \n",
    "            pred = model(xhat).argmax(dim=1)\n",
    "            is_correct = (pred == y)\n",
    "            preds_c.append(is_correct.cpu())\n",
    "            correct += is_correct.sum().item()\n",
    "            total += x.size(0)\n",
    "            acc = correct / total\n",
    "            pb.comment = f'Acc: {acc:.4f}'\n",
    "\n",
    "            for xi, xihat in zip(x,xhat):\n",
    "                x01 = xi.unsqueeze(0) / 2 + 0.5\n",
    "                xhat01 = xihat.unsqueeze(0) / 2 + 0.5\n",
    "                psnr.append(psnr_db(x01,xhat01))\n",
    "                ssim.append(ssim_01(x01,xhat01))\n",
    "                lpips.append(lpips_db(x01,xhat01))\n",
    "                dists.append(dists_db(x01,xhat01))\n",
    "    print(acc)\n",
    "    size_matrix.append(torch.tensor(sum(size_bytes,[])).unsqueeze(0))\n",
    "    correct_matrix.append(torch.cat(preds_c).unsqueeze(0))\n",
    "    psnr_matrix.append(torch.tensor(psnr).unsqueeze(0))\n",
    "    ssim_matrix.append(torch.tensor(ssim).unsqueeze(0))\n",
    "    lpips_matrix.append(torch.tensor(lpips).unsqueeze(0))\n",
    "    dists_matrix.append(torch.tensor(dists).unsqueeze(0))\n",
    "size_matrix = torch.cat(size_matrix)\n",
    "correct_matrix = torch.cat(correct_matrix)\n",
    "psnr_matrix = torch.cat(psnr_matrix)\n",
    "ssim_matrix = torch.cat(ssim_matrix)\n",
    "lpips_matrix = torch.cat(lpips_matrix)\n",
    "dists_matrix = torch.cat(dists_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714eab8-eb65-45f8-a7ea-6343ccdab5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input must be a 2D tensor.\")\n",
    "    nan_mask = torch.isnan(tensor).any(dim=0)\n",
    "    inf_mask = torch.isinf(tensor).any(dim=0)\n",
    "    bad_mask = nan_mask | inf_mask\n",
    "    return tensor[:, ~bad_mask]\n",
    "\n",
    "cr = 224*224*3/(size_matrix.float()).mean(dim=1)    \n",
    "psnr = drop_nan(psnr_matrix).mean(dim=1)\n",
    "lpips = drop_nan(lpips_matrix).mean(dim=1)\n",
    "ssim = ssim_matrix.mean(dim=1)\n",
    "dists = drop_nan(dists_matrix).mean(dim=1)\n",
    "acc = correct_matrix.float().mean(dim=1)\n",
    "plt.plot(psnr,acc,marker='.',label='PSNR')\n",
    "plt.plot(30*ssim+3,acc,marker='.',label='SSIM')\n",
    "plt.plot(lpips+17,acc,marker='.',label='LPIPS')\n",
    "plt.plot(dists+15,acc,marker='.',label='DISTS')\n",
    "plt.xlabel('Quality [various units]')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "print(f'CR:{cr}')\n",
    "print(f'PSNR:{psnr}')\n",
    "print(f'SSIM:{ssim}')\n",
    "print(f'LPIPS:{lpips}')\n",
    "print(f'DISTS:{dists}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d281a-974c-4929-85bd-f54160720b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_np = size_matrix.cpu().numpy()\n",
    "psnr_np = psnr_matrix.cpu().numpy()\n",
    "ssim_np = ssim_matrix.cpu().numpy()\n",
    "lpips_np = lpips_matrix.cpu().numpy()\n",
    "dists_np = dists_matrix.cpu().numpy()\n",
    "correct_np = correct_matrix.cpu().numpy().astype(int)\n",
    "dfs = []\n",
    "for i, resize in enumerate(resize_settings):\n",
    "    temp_df = pd.DataFrame({\n",
    "        'resize': resize,\n",
    "        'size_bytes': size_np[i],\n",
    "        'psnr': psnr_np[i],\n",
    "        'ssim': ssim_np[i],\n",
    "        'lpips': lpips_np[i],\n",
    "        'dists': dists_np[i],\n",
    "        'correct': correct_np[i]\n",
    "    })\n",
    "    dfs.append(temp_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4733ad-a11f-4db3-9e39-bdf05b956445",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = datasets.Dataset.from_pandas(df)\n",
    "results.push_to_hub('danjacobellis/imagenet_224_mpq2_liveaction_f16c48_lambdap1',split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd97db-be85-4eae-8fe2-057827a44bad",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g",
   "language": "python",
   "name": "g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
