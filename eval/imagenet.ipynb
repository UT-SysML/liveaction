{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375d3181-5231-450d-ba9e-6b3ad9fc3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, timm, datasets, fastprogress, io, PIL.Image, pillow_jpls\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from livecodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1c6732-51c2-4f1b-80f3-3dfe7937c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/g/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/g/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "codec_device = 'cuda:1'\n",
    "device = 'cuda:0'\n",
    "ssim_loss = SSIMLoss().to(device)\n",
    "lpips_loss = LPIPS().to(device)\n",
    "dists_loss = DISTS().to(device)\n",
    "psnr_db = lambda x01, xhat01: -10*torch.nn.functional.mse_loss(x01,xhat01).log10().item()\n",
    "ssim_01 = lambda x01, xhat01: 1 - ssim_loss(x01,xhat01).item()\n",
    "lpips_db = lambda x01, xhat01: -10*lpips_loss(x01,xhat01).log10().item()\n",
    "dists_db = lambda x01, xhat01: -10*dists_loss(x01,xhat01).log10().item()    \n",
    "\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/liveaction\",\n",
    "    filename=\"lsdir_f16c192.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "cconfig = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=cconfig.input_channels,\n",
    "    J = int(np.log2(cconfig.F)),\n",
    "    latent_dim=cconfig.latent_dim,\n",
    "    encoder_depth = cconfig.encoder_depth,\n",
    "    encoder_kernel_size = cconfig.encoder_kernel_size,\n",
    "    decoder_depth = cconfig.decoder_depth,\n",
    "    lightweight_encode = cconfig.lightweight_encode,\n",
    "    lightweight_decode = cconfig.lightweight_decode,\n",
    ").to(codec_device)\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "\n",
    "config = SimpleNamespace()\n",
    "config.batch_size = 16\n",
    "config.num_workers = 8\n",
    "config.inference_size = 224\n",
    "\n",
    "model = timm.create_model('timm/eva_giant_patch14_224.clip_ft_in1k',pretrained=True).to(device)\n",
    "dataset = datasets.load_dataset('danjacobellis/imagenet_1k_val_224',split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1b4809-bdb1-48ef-9000-0e614b28414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, resize):\n",
    "    B = len(batch)\n",
    "    y = []\n",
    "    x = []\n",
    "    xr = []\n",
    "    for sample in batch:\n",
    "        y.append(sample['cls'])\n",
    "        img = sample['crop224']\n",
    "        x.append(pil_to_tensor(img).unsqueeze(0))\n",
    "        img = img.resize((resize,resize), resample=PIL.Image.Resampling.BICUBIC)\n",
    "        xr.append(pil_to_tensor(img).unsqueeze(0))\n",
    "    x = torch.cat(x)\n",
    "    xr = torch.cat(xr)\n",
    "    return x, xr, torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b2ef66-f118-4762-b304-34e1da164117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8851\n"
     ]
    }
   ],
   "source": [
    "G_A = lambda x: codec.quantize.compand(codec.encode(x)).round()\n",
    "resize_settings = [224]\n",
    "\n",
    "correct_matrix = []\n",
    "psnr_matrix = []\n",
    "ssim_matrix = []\n",
    "lpips_matrix = []\n",
    "dists_matrix = []\n",
    "size_matrix = []\n",
    "\n",
    "mb = fastprogress.master_bar(resize_settings)\n",
    "for i_r, resize in enumerate(mb):\n",
    "    dataloader = torch.utils.data.dataloader.DataLoader(\n",
    "        dataset=dataset,\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=lambda batch: collate_fn(batch, resize),\n",
    "        batch_size=config.batch_size,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    preds_c = []\n",
    "    size_bytes = []\n",
    "    psnr = []\n",
    "    ssim = []\n",
    "    lpips = []\n",
    "    dists = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pb = fastprogress.progress_bar(dataloader,parent=mb)\n",
    "    \n",
    "    for i_batch, (x, xr, y) in enumerate(pb):\n",
    "        y = y.to(device)\n",
    "        x = x.to(torch.float).to(device)/127.5 - 1.0\n",
    "        xr = xr.to(torch.float).to(codec_device)/127.5 - 1.0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            z = G_A(xr)\n",
    "            compressed = latent_to_pil(z.cpu(), n_bits=8, C=3)\n",
    "            sb = []\n",
    "            for i_sample in range(config.batch_size):\n",
    "                buff = io.BytesIO()\n",
    "                compressed[i_sample].save(buff, format='JPEG-LS')\n",
    "                sb.append(len(buff.getbuffer()))\n",
    "                del buff\n",
    "            size_bytes.append(sb)\n",
    "            xhat = codec.decode(z).to(device)\n",
    "            xhat = Resize(config.inference_size,interpolation=PIL.Image.Resampling.BICUBIC)(xhat).clamp(-1,1)\n",
    "\n",
    "        with torch.inference_mode(): \n",
    "            pred = model(xhat).argmax(dim=1)\n",
    "            is_correct = (pred == y)\n",
    "            preds_c.append(is_correct.cpu())\n",
    "            correct += is_correct.sum().item()\n",
    "            total += x.size(0)\n",
    "            acc = correct / total\n",
    "            pb.comment = f'Acc: {acc:.4f}'\n",
    "\n",
    "            for xi, xihat in zip(x,xhat):\n",
    "                x01 = xi.unsqueeze(0) / 2 + 0.5\n",
    "                xhat01 = xihat.unsqueeze(0) / 2 + 0.5\n",
    "                psnr.append(psnr_db(x01,xhat01))\n",
    "                ssim.append(ssim_01(x01,xhat01))\n",
    "                lpips.append(lpips_db(x01,xhat01))\n",
    "                dists.append(dists_db(x01,xhat01))\n",
    "    print(acc)\n",
    "    size_matrix.append(torch.tensor(sum(size_bytes,[])).unsqueeze(0))\n",
    "    correct_matrix.append(torch.cat(preds_c).unsqueeze(0))\n",
    "    psnr_matrix.append(torch.tensor(psnr).unsqueeze(0))\n",
    "    ssim_matrix.append(torch.tensor(ssim).unsqueeze(0))\n",
    "    lpips_matrix.append(torch.tensor(lpips).unsqueeze(0))\n",
    "    dists_matrix.append(torch.tensor(dists).unsqueeze(0))\n",
    "size_matrix = torch.cat(size_matrix)\n",
    "correct_matrix = torch.cat(correct_matrix)\n",
    "psnr_matrix = torch.cat(psnr_matrix)\n",
    "ssim_matrix = torch.cat(ssim_matrix)\n",
    "lpips_matrix = torch.cat(lpips_matrix)\n",
    "dists_matrix = torch.cat(dists_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7714eab8-eb65-45f8-a7ea-6343ccdab5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR:tensor([5.6019])\n",
      "bpp:tensor([4.2843])\n",
      "PSNR:tensor([36.5948])\n",
      "LPIPS:tensor([14.8696])\n",
      "DISTS:tensor([15.5675])\n",
      "SSIM:tensor([0.9640])\n",
      "Acc:tensor([0.8851])\n"
     ]
    }
   ],
   "source": [
    "def drop_nan(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input must be a 2D tensor.\")\n",
    "    nan_mask = torch.isnan(tensor).any(dim=0)\n",
    "    inf_mask = torch.isinf(tensor).any(dim=0)\n",
    "    bad_mask = nan_mask | inf_mask\n",
    "    return tensor[:, ~bad_mask]\n",
    "\n",
    "cr = 224*224*3/(size_matrix.float()).mean(dim=1)\n",
    "bpp = 24/cr\n",
    "psnr = drop_nan(psnr_matrix).mean(dim=1)\n",
    "lpips = drop_nan(lpips_matrix).mean(dim=1)\n",
    "ssim = ssim_matrix.mean(dim=1)\n",
    "dists = drop_nan(dists_matrix).mean(dim=1)\n",
    "acc = correct_matrix.float().mean(dim=1)\n",
    "\n",
    "print(f'CR:{cr}')\n",
    "print(f'bpp:{bpp}')\n",
    "print(f'PSNR:{psnr}')\n",
    "print(f'LPIPS:{lpips}')\n",
    "print(f'DISTS:{dists}')\n",
    "print(f'SSIM:{ssim}')\n",
    "print(f'Acc:{acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666d281a-974c-4929-85bd-f54160720b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resize</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>psnr</th>\n",
       "      <th>ssim</th>\n",
       "      <th>lpips</th>\n",
       "      <th>dists</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224</td>\n",
       "      <td>25236</td>\n",
       "      <td>40.750114</td>\n",
       "      <td>0.979089</td>\n",
       "      <td>15.992846</td>\n",
       "      <td>17.092169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224</td>\n",
       "      <td>25442</td>\n",
       "      <td>40.497849</td>\n",
       "      <td>0.974480</td>\n",
       "      <td>17.589628</td>\n",
       "      <td>17.514608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224</td>\n",
       "      <td>24819</td>\n",
       "      <td>40.880730</td>\n",
       "      <td>0.980109</td>\n",
       "      <td>14.611721</td>\n",
       "      <td>15.917366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224</td>\n",
       "      <td>27455</td>\n",
       "      <td>34.401974</td>\n",
       "      <td>0.963233</td>\n",
       "      <td>15.255405</td>\n",
       "      <td>13.744248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224</td>\n",
       "      <td>28915</td>\n",
       "      <td>39.097141</td>\n",
       "      <td>0.990281</td>\n",
       "      <td>18.047140</td>\n",
       "      <td>18.316820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   resize  size_bytes       psnr      ssim      lpips      dists  correct\n",
       "0     224       25236  40.750114  0.979089  15.992846  17.092169        1\n",
       "1     224       25442  40.497849  0.974480  17.589628  17.514608        1\n",
       "2     224       24819  40.880730  0.980109  14.611721  15.917366        1\n",
       "3     224       27455  34.401974  0.963233  15.255405  13.744248        1\n",
       "4     224       28915  39.097141  0.990281  18.047140  18.316820        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_np = size_matrix.cpu().numpy()\n",
    "psnr_np = psnr_matrix.cpu().numpy()\n",
    "ssim_np = ssim_matrix.cpu().numpy()\n",
    "lpips_np = lpips_matrix.cpu().numpy()\n",
    "dists_np = dists_matrix.cpu().numpy()\n",
    "correct_np = correct_matrix.cpu().numpy().astype(int)\n",
    "dfs = []\n",
    "for i, resize in enumerate(resize_settings):\n",
    "    temp_df = pd.DataFrame({\n",
    "        'resize': resize,\n",
    "        'size_bytes': size_np[i],\n",
    "        'psnr': psnr_np[i],\n",
    "        'ssim': ssim_np[i],\n",
    "        'lpips': lpips_np[i],\n",
    "        'dists': dists_np[i],\n",
    "        'correct': correct_np[i]\n",
    "    })\n",
    "    dfs.append(temp_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4733ad-a11f-4db3-9e39-bdf05b956445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be57fba8ce2d40eaa041868de62fe24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19fb7eeedf34b509a754329ae3317e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67097e2490544185a6425effdbab95f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff793e9936fa4265a849c541134c0bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9cc28183524f2999f5ff31e495c05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  94%|#########3| 1.29MB / 1.38MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/danjacobellis/imagenet_224_mpq2_liveaction_f16c192/commit/10b50a80beb7e40a937e3ffb0425c9dfe672a082', commit_message='Upload dataset', commit_description='', oid='10b50a80beb7e40a937e3ffb0425c9dfe672a082', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/danjacobellis/imagenet_224_mpq2_liveaction_f16c192', endpoint='https://huggingface.co', repo_type='dataset', repo_id='danjacobellis/imagenet_224_mpq2_liveaction_f16c192'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = datasets.Dataset.from_pandas(df)\n",
    "results.push_to_hub('danjacobellis/imagenet_224_mpq2_liveaction_f16c192',split='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g",
   "language": "python",
   "name": "g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
