{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d657701f-a5e7-4f2b-aeb4-cc90175b2f67",
   "metadata": {},
   "source": [
    "LiVeAction f16c48\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ff5aed-d8b6-4a48-b3d9-2169e100c435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493.056\n"
     ]
    }
   ],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np, time\n",
    "from huggingface_hub import hf_hub_download\n",
    "from types import SimpleNamespace\n",
    "from livecodec.codec import AutoCodecND, latent_to_pil, pil_to_latent\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor, resize\n",
    "\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "checkpoint_file = hf_hub_download(\n",
    "    repo_id=\"danjacobellis/liveaction\",\n",
    "    filename=\"lsdir_f16c48.pth\"\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=\"cpu\",weights_only=False)\n",
    "config = checkpoint['config']\n",
    "codec = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J = int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    encoder_depth = 4,\n",
    "    encoder_kernel_size = config.encoder_kernel_size,\n",
    "    decoder_depth = config.decoder_depth,\n",
    "    lightweight_encode = config.lightweight_encode,\n",
    "    lightweight_decode = config.lightweight_decode,\n",
    ")\n",
    "codec.load_state_dict(checkpoint['state_dict'])\n",
    "codec.eval();\n",
    "print(sum( p.numel() for p in codec.encoder_blocks.parameters())/1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d4a20a-b7a8-4bf7-ba95-10d9fe22d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_throughput(sample, device='cpu', dtype=torch.float):\n",
    "    codec.to(device).to(dtype)\n",
    "    img = sample['image']\n",
    "    img = img.resize((int(2.5*img.size[0]),int(2.5*img.size[1]))) # 1080p\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(dtype) / 127.5 - 1.0\n",
    "    orig_size = tuple(x_orig.shape[-2:])\n",
    "    orig_dim = x_orig.numel() \n",
    "    \n",
    "    # analysis transform\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z = codec.encode(x_orig)\n",
    "        latent = codec.quantize.compand(z).round()\n",
    "    analysis_time = time.time() - t0\n",
    "    \n",
    "    # entropy coding\n",
    "    t0 = time.time()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    entropy_code_time = time.time() - t0\n",
    "    \n",
    "    # entropy decoding\n",
    "    t0 = time.time()\n",
    "    webp = [PIL.Image.open(buff)]\n",
    "    latent_decoded = pil_to_latent(webp, N=config.latent_dim, n_bits=8, C=3).to(device).to(dtype)\n",
    "    entropy_decode_time = time.time() - t0\n",
    "    \n",
    "    # synthesis transform\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        x_hat = codec.decode(latent_decoded).clamp(-1,1)\n",
    "    synthesis_time = time.time() - t0\n",
    "\n",
    "    return {\n",
    "        'analysis_time': analysis_time,\n",
    "        'entropy_code_time': entropy_code_time,\n",
    "        'entropy_decode_time': entropy_decode_time,\n",
    "        'synthesis_time': synthesis_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07fa45ee-c1e1-46a3-979d-da1b526a311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "166.79071482954996\n",
      "43.731614014788285\n",
      "754.5735253088093\n",
      "205.6265428837407\n",
      "34.647285170257454\n",
      "161.59168333836527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0deba2b251f468089fbeaa4b33919c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "10.539825783846718\n",
      "35.94067859807265\n",
      "762.0611611359977\n",
      "0.7378348391044605\n",
      "8.149836065984434\n",
      "0.7371211514191478\n"
     ]
    }
   ],
   "source": [
    "for (device,dtype) in [('cuda',torch.bfloat16),('cpu',torch.float)]:\n",
    "    results_dataset = dataset['validation'].map(lambda s: evaluate_throughput(s,device=device, dtype=dtype))\n",
    "    print(\"mean\\n---\")\n",
    "    for metric in [\n",
    "        'analysis_time',\n",
    "        'entropy_code_time',\n",
    "        'entropy_decode_time',\n",
    "        'synthesis_time',\n",
    "    ]:\n",
    "        μ = 1920*1080e-6/np.mean(results_dataset[metric])\n",
    "        print(μ)\n",
    "    print(1920*1080e-6/np.mean(np.array(results_dataset['analysis_time'])+np.array(results_dataset['entropy_code_time'])))\n",
    "    print(1920*1080e-6/np.mean(np.array(results_dataset['entropy_decode_time'])+np.array(results_dataset['synthesis_time'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ec4c1-e1c7-4da4-b71b-6d6ca2f5988d",
   "metadata": {},
   "source": [
    "Cosmos di16\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d369286-ad13-46e4-b5a9-dbcfda31acce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0209b9c6b2b24897b02353478a040019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.621062\n"
     ]
    }
   ],
   "source": [
    "import torch, io, datasets, PIL.Image,  numpy as np, time\n",
    "from huggingface_hub import snapshot_download\n",
    "from types import SimpleNamespace\n",
    "from piq import LPIPS, DISTS, SSIMLoss\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer\n",
    "\n",
    "dataset = datasets.load_dataset(\"danjacobellis/kodak\")\n",
    "model_path = snapshot_download(repo_id='nvidia/Cosmos-Tokenizer-DI16x16')\n",
    "encoder = ImageTokenizer(checkpoint_enc=f'{model_path}/encoder.jit')\n",
    "decoder = ImageTokenizer(checkpoint_dec=f'{model_path}/decoder.jit')\n",
    "\n",
    "lpips_loss = LPIPS()\n",
    "dists_loss = DISTS()\n",
    "ssim_loss = SSIMLoss()\n",
    "print(sum( p.numel() for p in encoder.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522b1fc4-05ed-4684-94d7-088e9a4e900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_throughput(sample, device='cuda', dtype=torch.bfloat16):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    img = sample['image']\n",
    "    img = img.resize((int(2.5*img.size[0]),int(2.5*img.size[1]))) # 1080p\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(dtype) / 127.5 - 1.0\n",
    "    orig_size = tuple(x_orig.shape[-2:])\n",
    "    orig_dim = x_orig.numel() \n",
    "    \n",
    "    # analysis transform\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z = encoder.encode(x_orig)[0]\n",
    "    analysis_time = time.time() - t0\n",
    "    \n",
    "    # entropy coding\n",
    "    t0 = time.time()\n",
    "    torch.save(z,'tmp.pth')\n",
    "    entropy_code_time = time.time() - t0\n",
    "    \n",
    "    # entropy decoding\n",
    "    t0 = time.time()\n",
    "    z = torch.load('tmp.pth')\n",
    "    entropy_decode_time = time.time() - t0\n",
    "    \n",
    "    # synthesis transform\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        x_hat = decoder.decode(z).to(torch.float).clamp(-1,1)\n",
    "    synthesis_time = time.time() - t0\n",
    "\n",
    "    return {\n",
    "        'analysis_time': analysis_time,\n",
    "        'entropy_code_time': entropy_code_time,\n",
    "        'entropy_decode_time': entropy_decode_time,\n",
    "        'synthesis_time': synthesis_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ddeb3b-053b-4beb-8463-eb08c3fc319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7ec51cd442c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52dd6b69bad454f9fad94f13dfe6f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "---\n",
      "20.852846693501366\n",
      "5862.201549852557\n",
      "6836.163312556494\n",
      "20.055054187077012\n",
      "20.778932500989743\n",
      "19.996391353291333\n"
     ]
    }
   ],
   "source": [
    "for (device,dtype) in [('cuda',torch.bfloat16)]:\n",
    "    results_dataset = dataset['validation'].map(lambda s: evaluate_throughput(s,device=device, dtype=dtype))\n",
    "    print(\"mean\\n---\")\n",
    "    for metric in [\n",
    "        'analysis_time',\n",
    "        'entropy_code_time',\n",
    "        'entropy_decode_time',\n",
    "        'synthesis_time',\n",
    "    ]:\n",
    "        μ = 1920*1080e-6/np.mean(results_dataset[metric])\n",
    "        print(μ)\n",
    "    print(1920*1080e-6/np.mean(np.array(results_dataset['analysis_time'])+np.array(results_dataset['entropy_code_time'])))\n",
    "    print(1920*1080e-6/np.mean(np.array(results_dataset['entropy_decode_time'])+np.array(results_dataset['synthesis_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88150e-85be-4dbf-a63a-30667ae35365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
